import json
import requests
from tqdm import tqdm
import random
import traceback
import time
import os

# APIé…ç½®
API_URL = 
API_KEY = 

def call_llm(prompt):
    """è°ƒç”¨LLM API"""
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {API_KEY}"
    }
    payload = {
        "model": "gpt-5.2",
        "messages": [
            {
                "role": "user",
                "content": prompt
            }
        ],
    }
    
    try:
        response = requests.post(API_URL, json=payload, headers=headers, timeout=600)
        
        if response.status_code == 200:
            result = response.json()
            if 'choices' in result and len(result['choices']) > 0:
                content = result['choices'][0]['message']['content']
                return content
        
        print(f"âš ï¸ APIå“åº”å¼‚å¸¸: çŠ¶æ€ç  {response.status_code}")
        return None
            
    except requests.exceptions.Timeout:
        print(f"âŒ è¯·æ±‚è¶…æ—¶")
        return None
    except Exception as e:
        print(f"âŒ APIè°ƒç”¨å¤±è´¥: {type(e).__name__} - {str(e)}")
        return None


def generate_single_batch(batch_num, num_samples=300):
    """
    ç”Ÿæˆå•ä¸ªæ‰¹æ¬¡çš„æ•°æ®
    
    :param batch_num: æ‰¹æ¬¡ç¼–å·
    :param num_samples: æœ¬æ‰¹æ¬¡è¦ç”Ÿæˆçš„æ•°é‡
    :return: æ•°æ®åˆ—è¡¨
    """
    
    categories = ["learning", "coding", "entertainment", "documentation", "social", "other"]
    
    prompt = f"""
ä½ æ˜¯Windowsæ´»åŠ¨è¿½è¸ªæ•°æ®ç”Ÿæˆä¸“å®¶ã€‚ç”Ÿæˆ {num_samples} æ¡çœŸå®çš„æ´»åŠ¨æ—¥å¿—ã€‚

åˆ†ç±»æ ‡å‡†ï¼ˆæ¯ç±»çº¦å {100//len(categories)}%ï¼‰ï¼š
1. learning: å­¦ä¹ èµ„æ–™ã€æ•™ç¨‹ã€è¯¾ç¨‹è§†é¢‘ã€æ•™å­¦ç½‘ç«™
2. coding: ç¼–ç¨‹è½¯ä»¶ã€ä»£ç ç¼–è¾‘ã€å‘½ä»¤è¡Œã€IDEï¼ˆCode.exe, python.exeï¼‰
3. entertainment: å¨±ä¹è§†é¢‘ã€æ¸¸æˆã€éŸ³ä¹ã€ç¤¾äº¤åª’ä½“æµè§ˆ
4. documentation: æ–‡æ¡£ç¼–è¾‘ã€æŠ¥å‘Šã€è¡¨æ ¼ã€PDFé˜…è¯»ï¼ˆWord.exe, Excel.exeï¼‰
5. social: å³æ—¶é€šè®¯ã€é‚®ä»¶ã€è§†é¢‘ä¼šè®®ï¼ˆQQ.exe, WeChat.exeï¼‰
6. other: ç³»ç»Ÿå·¥å…·ã€æ–‡ä»¶ç®¡ç†å™¨ã€æ¡Œé¢

è¦æ±‚ï¼š
- å„åˆ†ç±»åŸºæœ¬å‡åŒ€åˆ†å¸ƒ
- æµè§ˆå™¨çª—å£è¦è¦†ç›–ä¸åŒé¢†åŸŸ
- éœ€è¦å¢åŠ é¢å¤–çš„ä¸°å¯Œçš„æ¸¸æˆæ•°æ®ï¼ˆæ¯”å¦‚it takes two.exe ï¼Œ é­”å…½ä¸–ç•Œç­‰ï¼‰
- æ¯è¡Œä¸€æ¡JSONï¼Œä¸è¦ä»»ä½•é¢å¤–è¯´æ˜

æ ¼å¼ï¼š{{"process_name": "xxx.exe", "window_title": "xxx", "label": "xxx"}}
"""
    
    print(f"[æ‰¹æ¬¡ {batch_num}] ğŸ¤– è°ƒç”¨APIç”Ÿæˆ {num_samples} æ¡æ•°æ®...")
    response = call_llm(prompt)
    
    if not response:
        print(f"[æ‰¹æ¬¡ {batch_num}] âŒ APIè°ƒç”¨å¤±è´¥")
        return []
    
    # è§£æå“åº”
    dataset = []
    lines = response.strip().split('\n')
    
    for line in lines:
        line = line.strip()
        if not line or line.startswith('```') or line.startswith('#'):
            continue
        
        try:
            item = json.loads(line)
            if 'process_name' in item and 'window_title' in item and 'label' in item:
                if item['label'] not in categories:
                    item['label'] = 'other'
                item['text'] = f"{item['process_name']} | {item['window_title']}"
                dataset.append(item)
        except json.JSONDecodeError:
            continue
    
    print(f"[æ‰¹æ¬¡ {batch_num}] âœ… æˆåŠŸè§£æ {len(dataset)} æ¡æ•°æ®")
    return dataset


def generate_large_dataset_batched(total_batches=10, batch_size=300, output_file="dataset_large.jsonl"):
    """
    åˆ†æ‰¹ç”Ÿæˆå¤§é‡æ•°æ®
    
    :param total_batches: æ€»æ‰¹æ¬¡æ•°ï¼ˆé»˜è®¤10ï¼‰
    :param batch_size: æ¯æ‰¹ç”Ÿæˆæ•°é‡ï¼ˆé»˜è®¤300ï¼‰
    :param output_file: æœ€ç»ˆè¾“å‡ºæ–‡ä»¶
    """
    
    print("="*60)
    print(f"ğŸš€ åˆ†æ‰¹ç”Ÿæˆæ•°æ®é›†")
    print(f"   æ€»æ‰¹æ¬¡æ•°: {total_batches}")
    print(f"   æ¯æ‰¹æ•°é‡: {batch_size}")
    print(f"   ç›®æ ‡æ€»æ•°: {total_batches * batch_size}")
    print("="*60 + "\n")
    
    all_data = []
    
    # åˆ†æ‰¹ç”Ÿæˆ
    for batch_num in range(1, total_batches + 1):
        print(f"\n{'='*60}")
        print(f"å¼€å§‹æ‰¹æ¬¡ {batch_num}/{total_batches}")
        print(f"{'='*60}")
        
        batch_data = generate_single_batch(batch_num, batch_size)
        
        if batch_data:
            all_data.extend(batch_data)
            print(f"âœ… æ‰¹æ¬¡ {batch_num} å®Œæˆï¼Œç´¯è®¡: {len(all_data)} æ¡")
            
            # ä¿å­˜ä¸´æ—¶æ–‡ä»¶ï¼ˆé˜²æ­¢ä¸­é€”å¤±è´¥ï¼‰
            temp_file = f"temp_batch_{batch_num}.jsonl"
            with open(temp_file, 'w', encoding='utf-8') as f:
                for item in batch_data:
                    f.write(json.dumps(item, ensure_ascii=False) + '\n')
            print(f"ğŸ’¾ å·²ä¿å­˜ä¸´æ—¶æ–‡ä»¶: {temp_file}")
        else:
            print(f"âŒ æ‰¹æ¬¡ {batch_num} å¤±è´¥")
        
        # æ‰¹æ¬¡ä¹‹é—´æš‚åœï¼Œé¿å…APIé™æµ
        if batch_num < total_batches:
            print(f"â¸ï¸ æš‚åœ3ç§’...")
            time.sleep(3)
    
    print(f"\n{'='*60}")
    print("ğŸ“Š æ•°æ®å¤„ç†")
    print(f"{'='*60}")
    
    # å»é‡
    print(f"ğŸ“ å»é‡ä¸­...")
    seen = set()
    unique_data = []
    for item in all_data:
        key = (item['process_name'], item['window_title'], item['label'])
        if key not in seen:
            seen.add(key)
            unique_data.append(item)
    
    duplicate_count = len(all_data) - len(unique_data)
    print(f"âœ… å»é™¤ {duplicate_count} æ¡é‡å¤æ•°æ®")
    
    # ä¿å­˜æœ€ç»ˆæ–‡ä»¶
    print(f"\nğŸ’¾ ä¿å­˜æœ€ç»ˆæ•°æ®åˆ° {output_file}...")
    with open(output_file, 'w', encoding='utf-8') as f:
        for item in unique_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    print(f"âœ… ä¿å­˜æˆåŠŸ")
    
    # ç»Ÿè®¡
    print("\n" + "="*60)
    print("ğŸ“Š æœ€ç»ˆæ•°æ®é›†ç»Ÿè®¡")
    print("="*60)
    print(f"åŸå§‹æ•°æ®: {len(all_data)}")
    print(f"å»é‡å: {len(unique_data)}\n")
    
    categories = ["learning", "coding", "entertainment", "documentation", "social", "other"]
    label_counts = {cat: 0 for cat in categories}
    
    for item in unique_data:
        label = item.get('label', 'other')
        if label in label_counts:
            label_counts[label] += 1
        else:
            label_counts['other'] += 1
    
    for label, count in sorted(label_counts.items()):
        percentage = (count / len(unique_data) * 100) if unique_data else 0
        print(f"{label:15s} : {count:4d} ({percentage:.1f}%)")
    
    print("="*60)
    print(f"âœ… å®Œæˆï¼æ€»è®¡ {len(unique_data)} æ¡æ•°æ®")
    print(f"ğŸ’¾ ä¿å­˜ä½ç½®: {output_file}")
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    print(f"\nğŸ—‘ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
    for i in range(1, total_batches + 1):
        temp_file = f"temp_batch_{i}.jsonl"
        if os.path.exists(temp_file):
            os.remove(temp_file)
            print(f"  åˆ é™¤: {temp_file}")
    
    print("\nâœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼\n")
    
    return True


if __name__ == "__main__":
    print("="*60)
    print("ğŸš€ åˆ†æ‰¹æ•°æ®é›†ç”Ÿæˆå·¥å…·")
    print("="*60 + "\n")
    
    try:
        # ç”Ÿæˆæ•°æ®ï¼š10æ‰¹æ¬¡ï¼Œæ¯æ‰¹300æ¡
        generate_large_dataset_batched(
            total_batches=15,      # æ€»æ‰¹æ¬¡æ•°
            batch_size=500,        # æ¯æ‰¹æ•°é‡
            output_file="dataset_large.jsonl"
        )
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­")
        print("ğŸ’¡ æç¤º: ä¸´æ—¶æ–‡ä»¶å·²ä¿å­˜ï¼Œå¯ä»¥æ‰‹åŠ¨åˆå¹¶ temp_batch_*.jsonl æ–‡ä»¶")
    except Exception as e:
        print(f"\n\nâŒ ç¨‹åºå¼‚å¸¸: {type(e).__name__}")
        print(f"   è¯¦ç»†ä¿¡æ¯: {str(e)}")
        traceback.print_exc()