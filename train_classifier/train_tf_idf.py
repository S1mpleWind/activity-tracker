import json
import jieba
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import pickle

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç¬¬ä¸€æ­¥ï¼šä» dataset.jsonl è¯»å–æ•°æ®
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def load_dataset(file_path=""):
    """è¯»å–JSONLæ ¼å¼çš„æ•°æ®é›†"""
    texts = []
    labels = []
    
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            if not line.strip():
                continue
            item = json.loads(line)
            texts.append(item['text'])
            labels.append(item['label'])
    
    return texts, labels

print("ğŸ“– åŠ è½½æ•°æ®é›†...")
texts, labels = load_dataset("D:/files_n_data/learning/activity-tracker/dataset_large.jsonl")
print(f"âœ… æˆåŠŸåŠ è½½ {len(texts)} æ¡æ•°æ®\n")

# ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒ
print("ğŸ“Š æ ‡ç­¾åˆ†å¸ƒ:")
label_counts = {}
for label in labels:
    label_counts[label] = label_counts.get(label, 0) + 1

for label, count in sorted(label_counts.items()):
    print(f"  {label:15s} : {count:4d} ({count/len(labels)*100:.1f}%)")
print()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç¬¬äºŒæ­¥ï¼šæ–‡æœ¬é¢„å¤„ç†ï¼ˆä¸­æ–‡åˆ†è¯ï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def preprocess(text):
    """æ–‡æœ¬é¢„å¤„ç†ï¼šåˆ†è¯ + å»é™¤æ ‡ç‚¹"""
    words = jieba.cut(text)
    words = [w for w in words if w.strip() and w not in ["-", "|", ".", "ã€‚"]]
    return " ".join(words)

print("â³ æ–‡æœ¬é¢„å¤„ç†...")
processed_texts = [preprocess(t) for t in texts]
print("âœ… é¢„å¤„ç†å®Œæˆ\n")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç¬¬ä¸‰æ­¥ï¼šTF-IDF å‘é‡åŒ–
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("â³ TF-IDF å‘é‡åŒ–...")
vectorizer = TfidfVectorizer(
    max_features=1000,      # ä¿ç•™æœ€é‡è¦çš„1000ä¸ªè¯
    min_df=2,               # è‡³å°‘åœ¨2ä¸ªæ–‡æ¡£ä¸­å‡ºç°
    max_df=0.8,             # æœ€å¤šåœ¨80%æ–‡æ¡£ä¸­å‡ºç°
    ngram_range=(1, 2)      # ä½¿ç”¨1-gramå’Œ2-gram
)

X = vectorizer.fit_transform(processed_texts)
print(f"âœ… å‘é‡åŒ–å®Œæˆ: {X.shape[0]} æ ·æœ¬ Ã— {X.shape[1]} ç‰¹å¾\n")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç¬¬å››æ­¥ï¼šåˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("â³ åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†...")
X_train, X_test, y_train, y_test = train_test_split(
    X, labels, test_size=0.2, random_state=42, stratify=labels
)
print(f"âœ… è®­ç»ƒé›†: {X_train.shape[0]}, æµ‹è¯•é›†: {X_test.shape[0]}\n")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç¬¬äº”æ­¥ï¼šè®­ç»ƒåˆ†ç±»å™¨
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("â³ è®­ç»ƒé€»è¾‘å›å½’åˆ†ç±»å™¨...")
classifier = LogisticRegression(
    max_iter=1000,
    class_weight="balanced",
    random_state=42
)
classifier.fit(X_train, y_train)
print("âœ… è®­ç»ƒå®Œæˆ\n")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç¬¬å…­æ­¥ï¼šè¯„ä¼°æ¨¡å‹
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("="*60)
print("ğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœ")
print("="*60)

# è®­ç»ƒé›†å‡†ç¡®ç‡
train_score = classifier.score(X_train, y_train)
print(f"è®­ç»ƒé›†å‡†ç¡®ç‡: {train_score:.4f}")

# æµ‹è¯•é›†å‡†ç¡®ç‡
test_score = classifier.score(X_test, y_test)
print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_score:.4f}")

# è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
y_pred = classifier.predict(X_test)
print("\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, y_pred))

# æ··æ·†çŸ©é˜µ
print("æ··æ·†çŸ©é˜µ:")
cm = confusion_matrix(y_test, y_pred)
print(cm)
print("="*60 + "\n")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç¬¬ä¸ƒæ­¥ï¼šä¿å­˜æ¨¡å‹
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("ğŸ’¾ ä¿å­˜æ¨¡å‹...")
with open("model/classifier_model.pkl", "wb") as f:
    pickle.dump((vectorizer, classifier), f)
print("âœ… æ¨¡å‹å·²ä¿å­˜åˆ° model/classifier_model.pkl\n")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç¬¬å…«æ­¥ï¼šæµ‹è¯•é¢„æµ‹
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("="*60)
print("ğŸ§ª æµ‹è¯•é¢„æµ‹")
print("="*60 + "\n")

test_cases = [
    "Code.exe Visual Studio Code",
    "Chrome ç½‘é¡µæµè§ˆ",
    "Word å·¥ä½œæ–‡æ¡£",
    "QQ èŠå¤©å·¥å…·",
    "It takes two.exe åŒäººæˆè¡Œ",
    "League of Legends.exe è‹±é›„è”ç›Ÿ"
    "é­”å…½ä¸–ç•Œ é­”å…½"
]

for test_text in test_cases:
    processed = preprocess(test_text)
    vector = vectorizer.transform([processed])
    prediction = classifier.predict(vector)[0]
    probabilities = classifier.predict_proba(vector)[0]
    
    print(f"è¾“å…¥: {test_text}")
    print(f"é¢„æµ‹åˆ†ç±»: {prediction}")
    print(f"å„ç±»åˆ«æ¦‚ç‡:")
    
    for label, prob in sorted(
        zip(classifier.classes_, probabilities),
        key=lambda x: x[1],
        reverse=True
    ):
        print(f"  {label:15s} : {prob:.4f} ({prob*100:.2f}%)")
    print()

print("="*60)