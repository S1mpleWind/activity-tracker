import json
import fasttext
import os
from pathlib import Path

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç¬¬ä¸€æ­¥ï¼šä» dataset.jsonl è¯»å–æ•°æ®
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def load_dataset(file_path=""):
    """è¯»å–JSONLæ ¼å¼çš„æ•°æ®é›†"""
    texts = []
    labels = []
    
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            if not line.strip():
                continue
            item = json.loads(line)
            texts.append(item['text'])
            labels.append(item['label'])
    
    return texts, labels

print("ğŸ“– åŠ è½½æ•°æ®é›†...")
texts, labels = load_dataset("D:/files_n_data/learning/activity-tracker/dataset_large.jsonl")
print(f"âœ… æˆåŠŸåŠ è½½ {len(texts)} æ¡æ•°æ®\n")

# ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒ
print("ğŸ“Š æ ‡ç­¾åˆ†å¸ƒ:")
label_counts = {}
for label in labels:
    label_counts[label] = label_counts.get(label, 0) + 1

for label, count in sorted(label_counts.items()):
    print(f"  {label:15s} : {count:4d} ({count/len(labels)*100:.1f}%)")
print()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç¬¬äºŒæ­¥ï¼šç”Ÿæˆ FastText è®­ç»ƒæ–‡ä»¶
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("â³ ç”Ÿæˆ FastText è®­ç»ƒæ–‡ä»¶...")

# FastText æ ¼å¼ï¼š__label__<category> <text>
train_file = "fasttext_train.txt"
with open(train_file, 'w', encoding='utf-8') as f:
    for text, label in zip(texts, labels):
        # æ¸…ç†æ–‡æœ¬ï¼šå»é™¤æ¢è¡Œç¬¦
        clean_text = text.replace('\n', ' ').replace('\r', ' ')
        f.write(f'__label__{label} {clean_text}\n')

print(f"âœ… è®­ç»ƒæ–‡ä»¶å·²ç”Ÿæˆ: {train_file}\n")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç¬¬ä¸‰æ­¥ï¼šè®­ç»ƒ FastText æ¨¡å‹
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("â³ è®­ç»ƒ FastText æ¨¡å‹...")
print("  è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...\n")

model = fasttext.train_supervised(
    input=train_file,
    epoch=200,                    # è®­ç»ƒè½®æ•°ï¼ˆæ›´å¤šè½®æ•° = æ›´å¥½ä½†æ›´æ…¢ï¼‰
    lr=0.2,                      # å­¦ä¹ ç‡
    wordNgrams=3,                # ä½¿ç”¨ word n-gramï¼ˆ1-2 gramï¼‰
    dim=150,                     # è¯å‘é‡ç»´åº¦ï¼ˆè¶Šå¤§è¶Šå‡†ä½†è¶Šæ…¢ï¼‰
    minn=3,                      # æœ€å° char n-gram
    maxn=6,                      # æœ€å¤§ char n-gramï¼ˆsubword ç‰¹å¾ï¼‰
    loss='softmax',              # æŸå¤±å‡½æ•°
    bucket=200000,               # hash bucket æ•°é‡
    thread=4,                    # ä½¿ç”¨ 4 ä¸ªçº¿ç¨‹
    verbose=2                    # æ˜¾ç¤ºè®­ç»ƒè¿›åº¦
)

print("\nâœ… æ¨¡å‹è®­ç»ƒå®Œæˆ\n")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç¬¬å››æ­¥ï¼šè¯„ä¼°æ¨¡å‹
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("="*60)
print("ğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœ")
print("="*60)

# è·å–æ¨¡å‹é¢„æµ‹æ€§èƒ½æŒ‡æ ‡
N, precision, recall = model.test(train_file)

print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {N}")
print(f"ç²¾ç¡®ç‡ (Precision): {precision:.4f}")
print(f"å¬å›ç‡ (Recall): {recall:.4f}")
print(f"F1 å¾—åˆ†: {2 * precision * recall / (precision + recall):.4f}")
print("="*60 + "\n")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç¬¬äº”æ­¥ï¼šæµ‹è¯•é¢„æµ‹
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("="*60)
print("ğŸ§ª æ¨¡å‹é¢„æµ‹æµ‹è¯•")
print("="*60 + "\n")

test_cases = [
    "Code.exe Visual Studio Code",
    "Chrome ç½‘é¡µæµè§ˆ",
    "Word å·¥ä½œæ–‡æ¡£",
    "QQ èŠå¤©å·¥å…·",
    "It takes two.exe åŒäººæˆè¡Œ",
    "League of Legends.exe è‹±é›„è”ç›Ÿ",
    "é­”å…½ä¸–ç•Œ é­”å…½ä¸–ç•Œ",
    "Steam æ¸¸æˆå¹³å°",
    "Python æ•™ç¨‹å­¦ä¹ ",
    "Outlook å·¥ä½œé‚®ä»¶"
]

for test_text in test_cases:
    # é¢„æµ‹ï¼ˆè¿”å› label å’Œ confidenceï¼‰
    prediction = model.predict(test_text, k=3)  # è·å–å‰ 3 ä¸ªé¢„æµ‹
    labels_pred = prediction[0]
    scores = prediction[1]
    
    print(f"è¾“å…¥: {test_text}")
    print(f"é¢„æµ‹ç»“æœ (å‰ 3 ä¸ª):")
    
    for label, score in zip(labels_pred, scores):
        label_clean = label.replace('__label__', '')
        print(f"  {label_clean:15s} : {score:.4f} ({score*100:.2f}%)")
    
    print()

print("="*60)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç¬¬å…­æ­¥ï¼šä¿å­˜æ¨¡å‹
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("\nğŸ’¾ ä¿å­˜æ¨¡å‹...")

# åˆ›å»º model ç›®å½•
Path("model").mkdir(exist_ok=True)

# ä¿å­˜æ¨¡å‹ï¼ˆæœ‰ä¸¤ç§æ ¼å¼ï¼‰
model_path = "model/fasttext_model"

# 1. ä¿å­˜ä¸º .binï¼ˆåŒ…å«æ‰€æœ‰ä¿¡æ¯ï¼Œå¯ä»¥ç»§ç»­è®­ç»ƒï¼‰
    # å…ˆä¿å­˜ .binï¼Œç„¶åé‡åŒ–å‹ç¼©


model.save_model(f"{model_path}.bin")
print(f"âœ… æ¨¡å‹å·²ä¿å­˜ (bin æ ¼å¼): {model_path}.bin")

model.quantize(input=train_file, qnorm=True, retrain=False, cutoff=100000)
model.save_model(f"{model_path}_compressed.ftz")
print(f"âœ… æ¨¡å‹å·²ä¿å­˜ (ftz æ ¼å¼): {model_path}_compressed.ftz")

# è¾“å‡ºæ¨¡å‹å¤§å°
bin_size = os.path.getsize(f"{model_path}.bin") / (1024 * 1024)
print(f"\nğŸ“Š æ¨¡å‹å¤§å°:")
print(f"  .bin æ ¼å¼: {bin_size:.2f} MB")
print(f"  .ftz æ ¼å¼: çº¦ {bin_size * 0.3:.2f} MB (å‹ç¼©å)\n")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç¬¬ä¸ƒæ­¥ï¼šä½¿ç”¨ç¤ºä¾‹
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("="*60)
print("ğŸ’¡ ä½¿ç”¨å»ºè®®")
print("="*60)
print("""
1. å•ä¸ªé¢„æµ‹ï¼š
   prediction = model.predict("Code.exe Python")[0][0]
   # è¿”å›: '__label__learning'

2. æ‰¹é‡é¢„æµ‹ï¼š
   predictions = model.predict(["text1", "text2", "text3"])

3. è·å–ç½®ä¿¡åº¦ï¼š
   label, score = model.predict("text", k=1)
   confidence = score[0]

4. è·å–è¯å‘é‡ï¼š
   vector = model.get_word_vector("å­¦ä¹ ")

5. åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨ï¼ˆæ¨èç”¨ .ftz æ ¼å¼ï¼‰ï¼š
   model = fasttext.load_model('model/fasttext_model_compressed.ftz')
""")

print("="*60)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç¬¬å…«æ­¥ï¼šè¶…å‚æ•°è°ƒä¼˜å»ºè®®
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("\nğŸ”§ å¦‚æœæ•ˆæœä¸å¥½ï¼Œå°è¯•è°ƒæ•´ï¼š\n")
print("å‡†ç¡®ç‡å¤ªä½:")
print("  - å¢åŠ  epoch: 25 â†’ 50-100")
print("  - å¢åŠ  dim: 100 â†’ 150-200")
print("  - å¢åŠ  wordNgrams: 2 â†’ 3")
print()
print("æ¨¡å‹å¤ªå¤§:")
print("  - å‡å° dim: 100 â†’ 50-75")
print("  - å‡å° bucket: 200000 â†’ 100000")
print("  - ä½¿ç”¨ .ftz å‹ç¼©æ ¼å¼")
print()
print("æ¨ç†å¤ªæ…¢:")
print("  - å‡å° dim: 100 â†’ 50")
print("  - ä½¿ç”¨ .ftz å‹ç¼©æ ¼å¼")
print()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("\nğŸ—‘ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
if os.path.exists(train_file):
    os.remove(train_file)
    print(f"âœ… å·²åˆ é™¤: {train_file}")

print("\nâœ… è®­ç»ƒå®Œæˆï¼")