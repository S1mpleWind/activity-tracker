import json
import requests
from tqdm import tqdm
import re

# APIé…ç½®
API_URL = 
API_KEY =   # ä½¿ç”¨ä½ çš„APIå¯†é’¥

def call_gpt(prompt):
    """è°ƒç”¨GPT API"""
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {API_KEY}"
    }
    payload = {
        "model": "gpt-5.2",
        "messages": [
            {
                "role": "user",
                "content": prompt
            }
        ],
    }
    try:
        response = requests.post(API_URL, json=payload, headers=headers)
        return response.json()['choices'][0]['message']['content']
    except Exception as e:
        print(f"âŒ APIè°ƒç”¨å¤±è´¥: {e}")
        return None


def generate_labels_for_output(input_file="output.txt", output_file="dataset.jsonl"):
    """
    è¯»å– output.txtï¼Œç”¨GPTç”Ÿæˆå¯¹åº”çš„æ ‡ç­¾
    æ ¼å¼: è¿›ç¨‹å | çª—å£æ ‡é¢˜ -> è¿›ç¨‹å | çª—å£æ ‡é¢˜ | æ ‡ç­¾
    """
    print(f"ğŸ“– è¯»å–æ•°æ®æ–‡ä»¶: {input_file}")
    
    # è¯»å–æ•°æ®
    texts = []
    with open(input_file, 'r', encoding='utf-8') as f:
        texts = [line.strip() for line in f if line.strip()]
    
    if not texts:
        print("âŒ æ•°æ®æ–‡ä»¶ä¸ºç©º")
        return
    
    print(f"ğŸ“Š æ€»å…± {len(texts)} æ¡è®°å½•")
    
    # å®šä¹‰æ ‡ç­¾ç±»åˆ«
    categories = ["learning", "entertainment", "work", "social", "other"]
    
    # æç¤ºè¯æ¨¡æ¿
    prompt_template = f"""
ä½ æ˜¯ä¸€ä¸ªæ´»åŠ¨åˆ†ç±»ä¸“å®¶ã€‚æ ¹æ®ç»™å®šçš„è¿›ç¨‹åå’Œçª—å£æ ‡é¢˜ï¼Œå°†å…¶åˆ†ç±»åˆ°ä»¥ä¸‹ç±»åˆ«ä¹‹ä¸€ï¼š
{', '.join(categories)}

è§„åˆ™ï¼š
åˆ†ç±»æŒ‡å—ï¼š
1. learning:ä»£ç ç¼–è¾‘ã€å­¦ä¹ èµ„æ–™ã€æ•™ç¨‹ã€è¯¾ç¨‹è§†é¢‘ï¼Œæ•™å­¦ç½‘ç«™ç­‰
2. coding: ä½¿ç”¨ç¼–ç¨‹è½¯ä»¶æˆ–è€…åœ¨å‘½ä»¤è¡Œé‡Œæ‰§è¡Œå‘½ä»¤ï¼Œæˆ–è€…ç¼–è¾‘ä»£ç ï¼ˆCode.exe, python.exeè¿è¡Œç¼–ç¨‹ä»»åŠ¡ï¼‰
3. entertainment: è§†é¢‘ã€æ¸¸æˆã€éŸ³ä¹ã€å¨±ä¹ç½‘ç«™ç­‰ï¼ˆè§†é¢‘æ’­æ”¾å™¨ã€æ¸¸æˆå®¢æˆ·ç«¯ï¼‰
4. documentation: æ–‡æ¡£ç¼–è¾‘ã€æŠ¥å‘Šã€è¡¨æ ¼ã€å·¥ä½œè½¯ä»¶ç­‰ï¼ˆWord.exe, Excel.exe, PowerPoint.exe Pdfï¼‰
5. social: ç¤¾äº¤é€šè®¯ã€å³æ—¶æ¶ˆæ¯ã€é‚®ä»¶ç­‰ï¼ˆQQ.exe, WeChat.exe, Outlook.exeï¼‰
6. other: ç³»ç»Ÿå·¥å…·ã€æµè§ˆå™¨é—²ç½®ç­‰

è¯·ç›´æ¥è¿”å›åˆ†ç±»ç»“æœï¼Œåªè¿”å›ä¸€ä¸ªç±»åˆ«åç§°ï¼Œä¸è¦ä»»ä½•è§£é‡Šã€‚

è¿›ç¨‹å: {{process_name}}
çª—å£æ ‡é¢˜: {{window_title}}

åˆ†ç±»ç»“æœ:
"""
    
    # ç”Ÿæˆæ ‡ç­¾
    dataset = []
    print("ğŸ”„ æ­£åœ¨ç”Ÿæˆæ ‡ç­¾...\n")
    
    for text in tqdm(texts, desc="ç”Ÿæˆæ ‡ç­¾"):
        # è§£ææ–‡æœ¬
        if " | " not in text:
            continue
        
        process_name, window_title = text.split(" | ", 1)
        
        # è°ƒç”¨GPTç”Ÿæˆæ ‡ç­¾
        prompt = prompt_template.replace("{{process_name}}", process_name).replace("{{window_title}}", window_title)
        label = call_gpt(prompt)
        
        if label:
            # æ¸…ç†æ ‡ç­¾
            label = label.strip().lower()
            # æå–ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„ç±»åˆ«
            for cat in categories:
                if cat in label:
                    label = cat
                    break
            else:
                label = "other"
            
            # ä¿å­˜åˆ°æ•°æ®é›†
            dataset.append({
                "process_name": process_name,
                "window_title": window_title,
                "text": f"{process_name} {window_title}",
                "label": label
            })
    
    # ä¿å­˜æ•°æ®é›†
    print(f"\nğŸ’¾ ä¿å­˜æ•°æ®é›†åˆ°: {output_file}")
    with open(output_file, 'w', encoding='utf-8') as f:
        for item in dataset:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    # ç»Ÿè®¡ç»“æœ
    print("\n" + "="*50)
    print("ğŸ“Š æ ‡ç­¾ç»Ÿè®¡:")
    print("="*50)
    label_counts = {}
    for item in dataset:
        label = item['label']
        label_counts[label] = label_counts.get(label, 0) + 1
    
    for label, count in sorted(label_counts.items()):
        print(f"{label:15s} : {count:4d} ({count/len(dataset)*100:.1f}%)")
    print("="*50 + f"\nâœ… æˆåŠŸç”Ÿæˆ {len(dataset)} æ¡æ ‡è®°æ•°æ®\n")


def generate_batch_labels(input_file="output.txt", output_file="dataset.jsonl", batch_size=5):
    """
    æ‰¹é‡ç”Ÿæˆæ ‡ç­¾ï¼ˆä¸ºäº†æ•ˆç‡ï¼Œä¸€æ¬¡å¤„ç†å¤šæ¡ï¼‰
    """
    print(f"ğŸ“– è¯»å–æ•°æ®æ–‡ä»¶: {input_file}")
    
    # è¯»å–æ•°æ®
    texts = []
    with open(input_file, 'r', encoding='utf-8') as f:
        texts = [line.strip() for line in f if line.strip()]
    
    if not texts:
        print("âŒ æ•°æ®æ–‡ä»¶ä¸ºç©º")
        return
    
    print(f"ğŸ“Š æ€»å…± {len(texts)} æ¡è®°å½•")
    
    categories = ["learning", "entertainment", "work", "social", "other"]
    
    # æ‰¹é‡æç¤ºè¯
    batch_prompt_template = """
ä½ æ˜¯ä¸€ä¸ªæ´»åŠ¨åˆ†ç±»ä¸“å®¶ã€‚å¯¹ä»¥ä¸‹æ´»åŠ¨è¿›è¡Œåˆ†ç±»ï¼Œæ¯ä¸ªåˆ†ç±»åˆ° {categories} ä¹‹ä¸€ã€‚

è§„åˆ™ï¼š
1. learning: å­¦ä¹ ã€ç¼–ç¨‹ã€æ•™ç¨‹ç›¸å…³
2. entertainment: å¨±ä¹ã€è§†é¢‘ã€æ¸¸æˆç›¸å…³
3. work: å·¥ä½œã€æ–‡æ¡£ã€æŠ¥å‘Šç›¸å…³
4. social: ç¤¾äº¤ã€é€šè®¯ç›¸å…³
5. other: å…¶ä»–

è¯·æŒ‰ç…§JSONæ ¼å¼è¿”å›ç»“æœï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
[
  {{"text": "è¿›ç¨‹å çª—å£æ ‡é¢˜", "label": "åˆ†ç±»ç»“æœ"}},
  ...
]

éœ€è¦åˆ†ç±»çš„æ•°æ®ï¼š
{data}

è¯·ç›´æ¥è¿”å›JSONæ•°ç»„ï¼Œä¸è¦ä»»ä½•å…¶ä»–å†…å®¹ã€‚
"""
    
    dataset = []
    print("ğŸ”„ æ­£åœ¨æ‰¹é‡ç”Ÿæˆæ ‡ç­¾...\n")
    
    # åˆ†æ‰¹å¤„ç†
    for i in tqdm(range(0, len(texts), batch_size), desc="æ‰¹å¤„ç†"):
        batch = texts[i:i+batch_size]
        batch_data = "\n".join([f"- {text}" for text in batch])
        
        prompt = batch_prompt_template.format(
            categories=", ".join(categories),
            data=batch_data
        )
        
        response = call_gpt(prompt)
        
        if response:
            try:
                # æå–JSONéƒ¨åˆ†
                json_str = response[response.find('['):response.rfind(']')+1]
                results = json.loads(json_str)
                
                for result in results:
                    text = result.get('text', '')
                    label = result.get('label', 'other').lower()
                    
                    # éªŒè¯label
                    if label not in categories:
                        label = 'other'
                    
                    if " | " in text:
                        process_name, window_title = text.split(" | ", 1)
                    else:
                        process_name, window_title = text, ""
                    
                    dataset.append({
                        "process_name": process_name,
                        "window_title": window_title,
                        "text": text,
                        "label": label
                    })
            except json.JSONDecodeError:
                print(f"âš ï¸ æ— æ³•è§£æJSON: {response[:100]}")
                continue
    
    # ä¿å­˜æ•°æ®é›†
    print(f"\nğŸ’¾ ä¿å­˜æ•°æ®é›†åˆ°: {output_file}")
    with open(output_file, 'w', encoding='utf-8') as f:
        for item in dataset:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    # ç»Ÿè®¡ç»“æœ
    print("\n" + "="*50)
    print("ğŸ“Š æ ‡ç­¾ç»Ÿè®¡:")
    print("="*50)
    label_counts = {}
    for item in dataset:
        label = item['label']
        label_counts[label] = label_counts.get(label, 0) + 1
    
    for label, count in sorted(label_counts.items()):
        print(f"{label:15s} : {count:4d} ({count/len(dataset)*100:.1f}%)")
    print("="*50 + f"\nâœ… æˆåŠŸç”Ÿæˆ {len(dataset)} æ¡æ ‡è®°æ•°æ®\n")


if __name__ == "__main__":
    # é€‰æ‹©ä¸€ä¸ªæ–¹å¼è¿è¡Œ
    
    # æ–¹å¼1: é€æ¡ç”Ÿæˆæ ‡ç­¾ï¼ˆå‡†ç¡®ç‡é«˜ä½†é€Ÿåº¦æ…¢ï¼‰
    # generate_labels_for_output("output.txt", "dataset.jsonl")
    
    # æ–¹å¼2: æ‰¹é‡ç”Ÿæˆæ ‡ç­¾ï¼ˆé€Ÿåº¦å¿«ä½†å¯èƒ½å‡†ç¡®ç‡ç¨ä½ï¼‰
    generate_batch_labels("D:/files_n_data/learning/activity-tracker/train_classifier/output.txt", "dataset.jsonl", batch_size=5)